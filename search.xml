<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[mkdir()和mkdirs()的区别]]></title>
      <url>%2F2017%2F08%2F19%2Fmkdir()%E5%92%8Cmkdirs()%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[本文内容主要整理自Stack Overflow的问题：Difference between mkdir() and mkdirs() in java for java.io.File [closed] 问题起源最近在实习工作中，将项目部署到新环境的服务器中时，遇到了mkdir()失败的问题。原因是在项目中配置的路径信息在新服务器上不存在，而且该路径是多层目录结构，mkdir()在多层目录不存在的情况下会执行失败。但是如果手动去多个环境中挨个创建目录，工作实在是太繁琐。所以查资料寻找解决方案，这里发现了mkdirs()方法，顺利解决问题。看到stack overflow上对该问题的解释很XX，特此记录。 本篇文章重点不在技术难度，主要是做笔记记录，方便自己查阅和回顾。 问题描述java.io.FIle中file.mkDir() 和 file.mkDirs()的区别。 详解对于mkdirs()，它会同时创建该文件所在路径的所有缺失的父目录；而mkdir()只会创建该文件所在路径的最底层的目录，也就是该文件所在目录。当mkdir()无法找到即将创建的目录的上一层目录时，就会执行失败，返回false。 两个函数的返回值都是boolean类型。 举例来说，我需要在目录/tmp下创建新目录。首先我们有： 1File f = new File("/tmp/one/two/three"); 当我们调用函数mkdirs()： 1f.mkdirs(); 执行效果是同时创建了三层目录one、two、three，具体为： /tmp/one /tmp/one/two /tmp/one/two/three 对于mkdir()： 1File f = new File("/tmp/one/two/three").mkdir(); 当/tmp/one/two目录已经存在时，则执行成功，创建出目录：/tmp/one/two/three。而如果目录/tmp/one/two/three不存在，那么则会返回false。 延伸官方文档信息的源头还是官方文档和源码，可以继续看英文文档加深理解。 javadocs for mkdirs() javadocs for mkdir() linux命令对比这个命令可以类比Linux命令的mkdir。即，mkdir()函数效果类似于mkdir，而mkdirs()则类似于mkdir -p。 1mkdir -p /tmp/one/two/three 更详细的命令使用方式可以man进行查看。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[剑指Offer-字符串]]></title>
      <url>%2F2016%2F12%2F27%2F%E5%89%91%E6%8C%87Offer-%E5%AD%97%E7%AC%A6%E4%B8%B2%2F</url>
      <content type="text"><![CDATA[替换空格题目描述请实现一个函数，将一个字符串中的空格替换成 “%20”。例如，当字符串为 We Are Happy. 则经过替换之后的字符串为 We%20Are%20Happy。 时间限制：1 秒 空间限制：32768K 代码12345678910111213141516171819202122232425/** * @author Think * @since 2016-12-19 11:18:00 */public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; int tar = 32; String des = "%20"; StringBuffer reStr = new StringBuffer(); for (int i=0;i&lt;str.length();i++)&#123; char c = str.charAt(i); if (tar == c)&#123; reStr.append(des); &#125;else &#123; reStr.append(c); &#125; &#125; return reStr.toString(); &#125; public static void main(String[] args) &#123; StringBuffer in = new StringBuffer("We Are Happy"); System.out.println(new Solution().replaceSpace(in)); &#125;&#125; 《剑指Offer》版本-题目剑指Offer书本中的题目要求入参格式不一样，因此实现方法也不一样。 123void ReplaceBlank(char string[], int length)&#123; //TODO&#125; 就需要从后往前，逐字节移动替换字符。 《剑指Offer》-代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Solution &#123; public static void main(String[] args) &#123; int length = 20; char[] string = new char[length]; String str = "We are happy"; for (int i = 0; i &lt; str.length(); i++) &#123; string[i] = str.charAt(i); &#125; int pos = 0; while (string[pos]!='\0')&#123; System.out.print(string[pos++]); &#125; System.out.println(); Solution solution = new Solution(); solution.ReplaceBlank(string, length); pos = 0; while (string[pos]!='\0')&#123; System.out.print(string[pos++]); &#125; &#125; public void ReplaceBlank(char[] string, int length) &#123; if (string == null || length &lt;= 0) &#123; return; &#125; int originalLength = 0; int numberOfBlank = 0; int i = 0; while (string[i] != '\0') &#123; originalLength++; if (string[i] == ' ') &#123; numberOfBlank++; &#125; i++; &#125; int newLength = originalLength + 2 * numberOfBlank; // length为原始字符串数组的长度，如果替换后的总长度newLength超过length，则肯定无法替换成功。 if (newLength &gt; length) &#123; return; &#125; int indexOdOriginal = originalLength; int indexOfNew = newLength; while (indexOdOriginal &gt;= 0 &amp;&amp; indexOfNew &gt; indexOdOriginal) &#123; if (string[indexOdOriginal] == ' ') &#123; string[indexOfNew--] = '0'; string[indexOfNew--] = '2'; string[indexOfNew--] = '%'; &#125; else &#123; string[indexOfNew--] = string[indexOdOriginal]; &#125; indexOdOriginal--; &#125; &#125;&#125; 正则表达式匹配题目描述请实现一个函数用来匹配包括’.’和’*‘的正则表达式。模式中的字符’.’表示任意一个字符，而’*‘表示它前面的字符可以出现任意次（包含 0 次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串 “aaa” 与模式 “a.a” 和 “ab*ac*a” 匹配，但是与 “aa.a” 和 “ab*a” 均不匹配。 时间限制：1 秒 空间限制：32768K 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Solution &#123; public static void main(String[] args) &#123; char[] s = "aa".toCharArray(); char[] patten = "a*".toCharArray(); Solution solution = new Solution(); boolean result = solution.match(s, patten); System.out.println(result); &#125; public boolean match(char[] str, char[] pattern) &#123; if (str == null || pattern == null) &#123; return false; &#125; int i = 0, j = 0; return matchTwo(str, i, pattern, j); &#125; // 注意出现""/"."/"*"/".*"/a*a等情况，需要随时判断数组是否越界。 boolean matchTwo(char[] str, int i, char[] pattern, int j) &#123; int len1 = str.length; int len2 = pattern.length; // 如果Str和pattern匹配字符均已经到达末尾，则匹配成功。 if (i == len1 &amp;&amp; j == len2) &#123; return true; &#125; // 如果str还没结束，而pattern已经匹配结束，则匹配失败。 // 另一方面，如果str已经结束，而pattern位结束，不能判断匹配失败。 // 如：pattern以*结尾，可以匹配0位。 if (i &lt; len1 &amp;&amp; j == len2) &#123; return false; &#125; // 首先判断pattern的后一位是不是*：*会影响pattern前一位的判断。 // - 如果第二位是*，分一下几类。 // 1 匹配0位，则pattern后移2位。 // 2 字符串匹配1位成功，继续匹配下一位，pattern因为*存在，j不变； // 3 只匹配1位，pattern后移2位。 // 说实话，2和3非常相似。而且个人感觉有些情况同时适用于2和3， // 但可能只有一返回true，又因为通过||连接，从而结果为真。 // 求明确清晰的解析，点拨迷雾。 // 通过(j+1)&lt;len2和&amp;&amp;短路操作 限定pattern[j+1]不会数组越界。必须判断。 if ((j + 1) &lt; len2 &amp;&amp; pattern[j + 1] == '*') &#123; if ((i &lt; len1 &amp;&amp; pattern[j] == '.') || (i &lt; len1 &amp;&amp; str[i] == pattern[j])) &#123; return matchTwo(str, i, pattern, j + 2) || matchTwo(str, i + 1, pattern, j) || matchTwo(str, i + 1, pattern, j + 2); &#125; else &#123; return matchTwo(str, i, pattern, j + 2); &#125; &#125; else &#123; if ((i &lt; len1 &amp;&amp; pattern[j] == '.') || (i &lt; len1 &amp;&amp; str[i] == pattern[j])) &#123; return matchTwo(str, i + 1, pattern, j + 1); &#125; else &#123; return false; &#125; &#125; &#125;&#125; 字符流中第一个不重复的字符题目描述请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 时间限制：1秒 空间限制：32768K 这里的256是按照《剑指Offer》中说的8位的char类型。 代码12345678910111213141516171819202122232425262728293031323334353637public class Solution &#123; public static void main(String[] args) &#123; Solution solution = new Solution();// String str = "google";// String str = "";// String str = "aabbcc"; String str = "think"; for (int i=0;i&lt;str.length();i++)&#123; solution.Insert(str.charAt(i)); &#125; System.out.println(solution.FirstAppearingOnce()); &#125; int charTable[] = new int[256]; StringBuffer sb = new StringBuffer(); //Insert one char from stringstream public void Insert(char ch) &#123; sb.append(ch); if (ch &gt;= 256) &#123; return; &#125; charTable[ch]+=1; &#125; //return the first appearence once char in current stringstream public char FirstAppearingOnce() &#123; char defaultCh = '#'; char charStr[] = sb.toString().toCharArray(); for (int i=0;i&lt;charStr.length;i++)&#123; if (charTable[charStr[i]]==1)&#123; return charStr[i]; &#125; &#125; return defaultCh; &#125;&#125; 表示数值的字符串题目描述请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串 “+100”,”5e2”,”-123”,”3.1416” 和 “-1E-16” 都表示数值。 但是 “12e”,”1a3.14”,”1.2.3”,”+-5” 和 “12e+4.3” 都不是。 时间限制：1秒 空间限制：32768K 代码枚举方式，很多个if嵌套，感觉不是很好。很冗长。不过确实可以体现解题思路。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class Solution &#123; public static void main(String[] args) &#123; Solution solution = new Solution(); // True for: "+100","5e2","-123","3.1416" and "-1E-16" // False for: "12e","1a3.14","1.2.3","+-5" and "12e+4.3" // Other test case: +.123, 1+23, char[] str = "1+23".toCharArray(); System.out.println(solution.isNumeric(str)); &#125; public boolean isNumeric(char[] str) &#123; int numOfPoint = 0; if (str == null) &#123; return false; &#125; int index = 0; if (str[index] == '+' || str[index] == '-') &#123; index = 1; if (!isNumber(str[index]) &amp;&amp; str[index] != '.') &#123; return false; &#125; &#125; for (int i = index; i &lt; str.length; i++) &#123; if (str[i] == 'e' || str[i] == 'E') &#123; if ((i + 1) &lt; str.length &amp;&amp; (str[i + 1] == '+' || str[i + 1] == '-') &amp;&amp; (i + 2) &lt; str.length &amp;&amp; isNumber(str[i + 2])) &#123; index = i + 3; break; &#125; else if ((i + 1) &lt; str.length &amp;&amp; isNumber(str[i + 1])) &#123; index = i + 2; break; &#125; else &#123; return false; &#125; &#125; else if (str[i] == '.') &#123; numOfPoint++; if (numOfPoint &gt; 1) &#123; return false; &#125; &#125; else &#123; if (!isNumber(str[i])) &#123; return false; &#125; &#125; index = i; &#125; for (int i = index; i &lt; str.length; i++) &#123; if (!isNumber(str[i])) &#123; return false; &#125; &#125; if (numOfPoint &gt; 1) &#123; return false; &#125; return true; &#125; public boolean isNumber(char ch) &#123; if (ch &lt; '0' || ch &gt; '9') &#123; return false; &#125; else &#123; return true; &#125; &#125;&#125; 看到有人的解题方法是使用Double.parseDouble(new String(str));方法，感觉不是很好，如果字符串表示的数值范围超出了Double呢？！ 代码21234567public class Solution &#123; public boolean isNumeric(char[] str) &#123; String s=String.valueOf(str); //return s.matches("[+-]?[0-9]*(.[0-9]*)?([eE][+-]?[0-9]+)?"); return s.matches("[+-]?[0-9]*(\\.[0-9]*)?([eE][+-]?[0-9]+)?"); &#125;&#125; 简洁、简单明了。 有人提到可以使用“编译原理中自动机”实现，思路条理清晰，赞。 小结注意上述几个问题的边界问题处理方式。null、空字符串、不符合要求的、符合要求的、多重符合题目要求的，均可以自己写出相应的字符串进行测试，防止程序出现异常而崩溃。 题目简单并不意味着代码简单。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Django速成-构建一个Blog-Demo]]></title>
      <url>%2F2016%2F11%2F26%2FDjango%E9%80%9F%E6%88%90-%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AABlog%20Demo%2F</url>
      <content type="text"><![CDATA[根据书本引导构建的一个Django Blog Demo，还很简陋。主要为引导用户熟悉Django的流程，以及运作原理。 主要设置项为setting.py文件。具体的更改数据库，配置数据库参数，时区等都可以在这里配置。更多设置项可以参考文件里的注释链接查阅官方文档进行更改。 Python版本：3.5.2，Django版本：(1, 9, 7, ‘final’, 0)。 运行配置完是数据库后，首先在manage.py统计目录下运行下列两个命令使数据库生效。 1$ manage.py makemigrations [appname] #appname &#21363;&#20026;&#27492;&#22788;&#30340;Blog&#10;$ manage.py migrate 这里就可以运行了。 运行Django自带的服务器查看效果： 1$ manage.py runserver 在浏览器输入：http://localhost:8000/blog/访问页面查看效果，默认浏览器端口是8000。 admin后台Django自带强大的后台功能，和ORM框架，因此基本可以满足后台管理功能，结合官方文档的指导可以使用更多地功能来配合官方场景。如果觉得后台管理功能满足不了自己的需求，可以自行实现后台系统，在此之前还是首先查看文档是否能有解决方案。 1$ manage.py createsuperuser 在runserver之前，先运行上面的命令为自己创建一个后台管理员账号和密码，比如这里的root和密码rootroot。 然后就可以通过http://localhost:8000/admin/来访问后台。 代码附录代码下载地址。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python生成指定大小文件]]></title>
      <url>%2F2016%2F11%2F19%2FPython%E7%94%9F%E6%88%90%E6%8C%87%E5%AE%9A%E5%A4%A7%E5%B0%8F%E6%96%87%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[在平时做实验的过程中，经常需要生成一些测试数据，比如指定文件大小、文件记录数，以及文本型、数字型数据等要求。本文记录使用Python生成指定大小的文件和生成指定数量的数据文件两种方式，以及另一个Python小程序，可以读取文件验证文件的行数，即记录数。 以下程序均为Python3版本。 生成指定大小的文件12345678910111213141516171819202122#!/usr/bin/env python#-*- coding:utf-8 -*-import osimport randomdef genSizeFile(fileName, fileSize): #file path filePath="Data"+fileName+".txt" # 生成固定大小的文件 # date size ds=0 with open(filePath, "w", encoding="utf8") as f: while ds&lt;fileSize: f.write(str(round(random.uniform(-1000, 1000),2))) f.write("\n") ds=os.path.getsize(filePath) # print(os.path.getsize(filePath))# start here.genSizeFile("1k",1*1024) 通过f.write()往文件内写入数据，这里写入的数据是-1000~1000内的随机数浮点数，保留两位小数点。写入的时候如果不是String类型的数据，必须通过str()转换。 生成指定数量的数据文件123456789101112131415161718192021#!/usr/bin/env python#-*- coding:utf-8 -*-import osimport randomdef genNFile(fileNum): numCount=fileNum numRange=3*numCount tmpList=random.sample(range(numRange), numCount) i=0 filePath=""+str(numCount)+".txt" with open(filePath, "w", encoding="utf8") as f: while i&lt;numCount: f.write(str(tmpList[i])) f.write("\n") i=i+1genNFile(11000) 文件内容随机，且不重复。 查看文件记录数主要思想在于统计文件内换行符\n的数量，即为文件的行数。 程序设置了buffer缓存数据，具体大小可以根据自己的文件大小进行调节。 1234567891011121314151617#!/usr/bin/env python#-*- coding:utf-8 -*-def countLine(thefilepath): count = 0 with open(thefilepath, 'rb') as thefile: while True: buffer = thefile.read(100*1024*1024) if not buffer: break count += str(buffer, encoding = "utf8").count('\n') print(count)# thefile.close( )# here.path="Data"+"1G"+".txt"countLine(path) 小结关于Python的资料很多，生成指定大小文件的方式也有，但是部分资料生成的数据为无意义文本，不适用于某些特定场景。本文记录的程序也不一定适合所有人，不过可以根据已有的程序和实现思路，举一反三，引申、拓展出自己想要的效果。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark入门 - 常用Spark监控Tab]]></title>
      <url>%2F2016%2F11%2F16%2FSpark%E5%85%A5%E9%97%A8%20-%204%E4%B8%AA%E5%B8%B8%E7%94%A8Spark%E7%9B%91%E6%8E%A7Tab%2F</url>
      <content type="text"><![CDATA[常用Spark监控Tab最近用Spark做任务，中间来回配置集群环境，查看配置后的效果，以及监测程序运行过程中的运行进度等，需要频繁查看WebUI的几个Tab。各个tab功能不一，从不同方面显示了Spark的各方面性能参数和运行进度。 特意记录一下，方便以后用得到的时候能够快速回顾知识点。 HDFS配置概览第一个tab是在配置好Hadoop之后就可以查看的。在这之前要先运行start-dfs.sh开启文件系统。具体地址为http://192.168.247.130:50070/dfshealth.html#tab-overview。配置完成后 在浏览器中输入该地址，或直接输入http://192.168.247.130:50070也可以自动跳转至该界面，界面效果如下： 在这里可以查看集群配置的具体参数，如硬盘大小，使用率、堆栈大小、内存大小等信息。如果自行配置了某个配置项，而又不确定是否已经生效，则可以在这里查看结果。 另一方面，结合Spark配置集群的时候，如果master节点如前几篇文章的配置，则master节点只负责集群任务调度，并不参与文件的存储和计算任务。因此在这里的Datanode标签里只能查看到Work1和Work2节点。如果需要将集群的所有资源都调度起来参与存储和计算，则可以更改配置后，查看此Tab确认配置是否生效。为达到调度所有集群的目的，可以和Spark的某一tab配置查看。 HDFS文件系统Spark的很多计算都是先从外部读取文件后转换成RDD然后才开始RDD转换和Action操作，因此前期很高频的一个操作就是将文件上传至HDFS文件系统中存储。上传命令不多说，官方文档、各类博客都可以学习参考。这里介绍的第二个tab即为查看HDFS上的文件，具体地址为http://192.168.247.130:50070/explorer.html#/，具体效果如下： 在这里我们可以看到HDFS中的文件和文件夹、文件大小、Block Size等信息，以及上文Spark History Server配置一文中设置的History文件夹。 HDFS默认的Block Size为64M和128M。在做Spark并行实验的时候，可以通过修改配置文件永久改变，或上传文件的时候手动设置临时改变Block Size的大小。 (1) 修改配置文件永久改变 修改配置文件hdfs-site.xml，在之前配置文件的基础上，加上下面的配置： 1234&lt;property&gt; &lt;name&gt;dfs.block.size&lt;/name&gt; &lt;value&gt;33554432&lt;/value&gt; &lt;/property&gt; 该配置的缩进级别与dfs.replication处于同一级。value的值33554432为32M=32*1024*1024，只能是确定值如33554432，不能是计算式32*1024*1024。 重新修改value值即可恢复到之前的设置。 (2) 在上传命令中临时设置 上传命令方法简单，具体命令为： 1hadoop dfs -D dfs.blocksize=5242880 -put /home/hadoop/think/Data100M.txt /think/ Spark配置概览与第一个tab的Hadoop概览类似，文中的第三个tab是Spark配置集群成功后，查看Spark相关配置项的地方，如工作节点的数量、集群可用核心数、可用内存、以及各个节点的简略信息。方法也是先开启HDFS，然后运行start-all.sh开启Spark。具体地址为http://192.168.247.130:8080/，展示效果如下图： 配合第一个Tab可以查看集群中各个工作节点的基本信息，方便做出相应的更改。 应用程序运行期间的WebUI监控查看具体的运行时间，Job、Task数量和运行阶段，以及详细的每个Task在节点的运行时间，输入输出文件大小等信息，并且可以通过图形界面直观查看节点的运行调度信息。还可以看到DAG信息，虽然不确定是否十分准确。在Environment标签查看更加详细的信息。 具体地址为http://192.168.247.130:4040/。查看到的效果为： 注：图片为网络资源，侵权的话请联系本人删除。谢谢 应用程序运行结束后的WebUI监控在文章Spark History Server配置一文中已经介绍过Spark运行期间和运行结束后的WebUI监控界面。这里的第5个tab就是之前介绍的Spark History Server界面信息。具体内容与4040端口一致。 具体地址为http://192.168.247.130:18080/，展示效果如图： 小结以上5个tab即为Spark配置和运行前后的一些信息查看源。熟练使用可以事半功倍，准确有效地调度集群资源，便于优化。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark入门 - History Server配置使用]]></title>
      <url>%2F2016%2F11%2F13%2FSpark%E5%85%A5%E9%97%A8%20-%20history%20server%2F</url>
      <content type="text"><![CDATA[问题描述在Spark安装成功后，无论是通过spark-submit工具还是通过Intellij IDEA提交任务，只要在Spark应用程序运行期间，都可以通过WebUI控制台页面来查看具体的运行细节，在浏览器中通过地址：http://&lt;driver-node&gt;:4040即可查看当前的运行状态。但是一旦应用程序运行结束，该Web界面也就失效了，无法继续查看监控集群信息。无法回顾刚刚运行的程序细节，进而定向做出优化，肯定极为不便。 这时候就需要为集群配置Spark History Server了。 注：&lt;driver-node&gt;可以是主机名，如master，也可以是主机名对应的IP。 Spark History ServerSpark History Server可以很好地解决上面的问题。 通过配置，我们可以在Spark应用程序运行完成后，将应用程序的运行信息写入知道目录，而Spark History Server可以将这些信息装在并以Web形式供用户浏览。 要使用Spark History Server，对于提交应用程序的客户端需要配置以下参数。 Spark History Server配置下面以Standalone模式为例说明配置信息。更多扩展细节可以根据自己的需求在此基础上添加配置项。 1、首先配置$SPARK_HOME$/conf目录下的spark-defaults.conf文件。 默认spark-defaults.conf是不存在的，我们可以根据Spark提供的template文件新建之。 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# mv spark-defaults.conf.template spark-defaults.conf&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# vim spark-defaults.conf 并修改文件内容为： 1spark.eventLog.enabled true&#10;spark.eventLog.dir hdfs://master:9000/history&#10;spark.eventLog.compress true 属性说明 (1) spark.eventLog.enabled。 是否记录Spark事件，用于应用程序在完成后的筹够WebUI。 (2) spark.eventLog.dir。 设置spark.eventLog.enabled为true后，该属性为记录spark时间的根目录。在此根目录中，Spark为每个应用程序创建分目录，并将应用程序的时间记录到此目录中。用户可以将此属性设置为HDFS目录，以便History Server读取。 (3) spark.eventLog.compress。 否压缩记录Spark事件，前提spark.eventLog.enabled为true，默认使用的是snappy。 2、在HDFS中建立存放目录。 上文已经设置好了存放History的目录HDFS文件目录，现在我们在HDFS中建立相应的目录专门存放文件。 1root@master:~# hadoop dfs -mkdir /history 之后的历史记录都会被存放到这里。 3、配置spark-env.sh文件。 在之前的配置项后面 1export SPARK_HISTORY_OPTS=&#34;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://master:9000/history&#34; 属性说明 (1) spark.history.ui.port WebUI的端口号。默认为18080，也可以自行设置。 (2) spark.history.retainedApplications 设置缓存Cache中保存的应用程序历史记录的个数，默认50，如果超过这个值，旧的将被删除。 注：缓存文件数不表示实际显示的文件总数。只是表示不在缓存中的文件可能需要从硬盘读取，速度稍有差别。 (3) spark.history.fs.logDirectory 存放历史记录文件的目录。可以是Hadoop APIs支持的任意文件系统。 更多配置参考文章末尾的链接。 启动依次启动Hadoop的start-dfs.sh和Spark的start-all.sh后，再运行start-history-server.sh文件即可启动历史服务。 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/sbin# ./start-history-server.sh 如果提示文件目录无效，可以直接使用下面的命令指定目录： 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/sbin# ./start-history-server.sh hdfs://master:9000/history 启动后的效果如下（此时尚未运行程序，没有记录显示出来）： 之后在每次应用程序运行结束后，就可以在这里观察刚刚程序的细节了。 小结应用程序运行时的4040和历史记录的18080WebUI配合使用，可以让我们在运行Spark应用的时候随时监测程序运行状态，并作相应的优化和调节，效果显著。 小问题不过其中也有一部分小细节尚未解决：如某次程序允许的时候用户选择自行中断，但历史记录中仍然存在该记录，而且最主要的是目前无法删除单项纪录。这就显得很繁琐了，如果需要对多项历史记录进行对比分析，就不得不先在很多的记录中筛选出哪些是有用记录，哪些是无用记录，费时费力。 虽然官方提供了如spark.history.fs.cleaner.interval和spark.history.fs.cleaner.maxAge配置项，但依然不能很好地解决这一问题。 期待后续有更好的解决方案出现。 参考文章 Spark Monitoring and Instrumentation—http://spark.apache.org/docs/latest/monitoring.html ​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark入门 - 3 测试Spark集群]]></title>
      <url>%2F2016%2F07%2F26%2FSpark%E5%85%A5%E9%97%A8%20-%203%20%E6%B5%8B%E8%AF%95Spark%E9%9B%86%E7%BE%A4%2F</url>
      <content type="text"><![CDATA[这是Spark入门的第三篇，也是最后一篇。说是入门，其实就只是简单的根据两本Spark书本，结合网络资料，自己一边动手实现，一边做的记录。自己看的时候确实方便许多，重新搭建一遍Hadoop-2.6.4的时候，查看前两篇记录，重现一遍的时候效率很高。 环境的搭建只是基础中的基础，不过之后的一切工作都要在这上面展开，并且用途也因人而异。因此这部分的记录入门尚可。 通过Spark提供的示例LocalPi测试Spark集群该示例是用Spark的run-example命令在Spark集群里运行示例LocalPi，最终打印Pi的一个大约的值到Shell控制台。 （1）启动Spark集群和Spark Shell （2）进入Spark的bin目录下，用run-example命令运行Spark自带的示例LocalPi，该示例的源码如下： 123456789101112131415161718package org.apache.spark.examplesimport scala.math.randomimport org.apache.spark._import org.apache.spark.SparkContext._object LocalPi &#123; def main(args: Array[String]) &#123; var count = 0 for (i &lt;- 1 to 100000) &#123; val x = random * 2 - 1 val y = random * 2 - 1 if (x*x + y*y &lt; 1) count += 1 &#125; println("Pi is roughly " + 4 * count / 100000.0) &#125;&#125; 在master结点的Spark的bin目录下输入以下命令： 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6# ./bin/run-example org.apache.spark.examples.LocalPi spark://master:7077&#10;Pi is roughly 3.14716&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6# 结语根据书本整理的三篇Spark入门文章到此结束。大致实现思路均为本人亲自实现，修正了书本中的一些错误。 更为详细的使用，以及一些理论知识，还需要进一步学习！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark入门 - 2 Spark安装和集群部署]]></title>
      <url>%2F2016%2F07%2F25%2FSpark%E5%85%A5%E9%97%A8%20-%202%20Spark%E5%AE%89%E8%A3%85%E5%92%8C%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
      <content type="text"><![CDATA[安装Scala版本选择Spark官方对配套的Scala版本有规定，所以要根据自己的实际情况来选择Scala版本。因此首先去Spark官网下载Spark，再根据要求下载对应版本的Scala。 在http://spark.apache.org/docs/1.6.2/中有一句提示： 1Spark runs on Java 7+, Python 2.6+ and R 3.1+. For the Scala API, Spark 1.6.2 uses Scala 2.10. You will need to use a compatible Scala version (2.10.x). 意味着Spark 1.6.2版本最好选择Scala 2.10.x。当然也可以选择更高版本的2.11.x，比如2.11.8，不过需要手动编译。 结合上一篇文章，Hadoop和Spark版本最好也要适配，选择pre-build版本。也可以选择source code或者自己指定Hadoop版本，这部分知识本文不再介绍。 此处版本为spark-1.6.2-bin-hadoop2.6和scala-2.10.6，还有hadoop-2.6.4版本。注意自己实现的时候版本兼容选择。 安装1、 将下载的压缩包解压后放入指定位置，usr/lib/scala。 1root@master:~/Documents# ls&#10;hadoop-2.4.6.tar.gz scala-2.10.6.tgz spark-1.6.2-bin-hadoop2.6.tgz&#10;root@master:~/Documents# tar zxvf scala-2.10.6.tgz&#10;......&#10;root@master:~/Documents# mkdir /usr/lib/scala&#10;root@master:~/Documents# mv scala-2.10.6 /usr/lib/scala/&#10;root@master:~/Documents# cd /usr/lib/scala/&#10;root@master:/usr/lib/scala# ls&#10;scala-2.10.6&#10;root@master:/usr/lib/scala# 2、 配置环境变量 打开~/.bashrc文件（默认是root用户登录，因此此处绝对路径为/root/.bashrc，前文中没有特殊说明的情况下均采用这里的标准），配置Scala环境变量。主要为SCALA_HOME和PATH。 1#if [ -f /etc/bash_completion ] &#38;&#38; ! shopt -oq posix; then&#10;# . /etc/bash_completion&#10;#fi&#10;export JAVA_HOME=/usr/lib/java/jdk1.8.0_91&#10;export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$SCALA_HOME/bin:$PATH&#10;export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar&#10;export JRE_HOME=$&#123;JAVA_HOME&#125;/jre&#10;&#10;export HADOOP_HOME=/usr/local/hadoop/hadoop-2.4.6&#10;export SCALA_HOME=/usr/lib/scala/scala-2.10.6 然后root@master:~# source .bashrc，使配置生效。 注意$SCALA_HOME这种书写方式，书本是使用${SCALA_HOME}。但是scala -version失败。这里还是自己测试一下，一种不行再试另一种，多折腾。 3、 测试配置是否成功 在终端输入scala -version查看Scala的版本信息。 不成功的话，再返回上面修改。成功后，可以直接使用scala进入Scala的命令交互界面。 1root@master:~# scala&#10;Welcome to Scala version 2.10.6 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91).&#10;Type in expressions to have them evaluated.&#10;Type :help for more information.&#10;&#10;scala&#62; 3*4&#10;res0: Int = 12&#10;&#10;scala&#62; 到这里基本安装成功。master主机已经配置好，其余主机也做相应配置即可。 4、 迁移到其他主机 另外两台worker主机也需要与master主机保持一致。因此考虑使用scp命令复制Scala文件夹和~/.bashrc文件到其余主机。 1root@master:/usr/lib# scp -r scala/ root@worker1:/usr/lib/&#10;root@master:/usr/lib# scp ~/.bashrc root@worker1:~&#10;root@master:/usr/lib# scp -r scala/ root@worker2:/usr/lib/&#10;root@master:/usr/lib# scp ~/.bashrc root@worker2:~ 记得最后在worker主机上source让配置文件生效。 当然也可以手动配置，重复上述过程。 最后测试一下其余主机Scala是否可用。 安装Spark和集群部署Spark需要运行在三台机器上，这里先安装Spark到master主机上，另外两台的安装方法一样，也可以使用SSH的scp命令把master主机上的安装好的Spark目录复制到另外两台机器相同的目录下。 1、 将下载的Spark放置在指定目录：/usr/local/spark。 按照之前的描述，下载适配版本的Spark并解压。如此处的spark-1.6.2-bin-hadoop2.6.tgz。 1root@master:~/Documents# tar zxvf spark-1.6.2.tgz&#10;...&#10;root@master:~/Documents# mkdir /usr/local/spark&#10;root@master:~/Documents# ls&#10;hadoop-2.4.6.tar.gz scala-2.10.6.tgz spark-1.6.2-bin-hadoop2.6 spark-1.6.2-bin-hadoop2.6.tgz&#10;root@master:~/Documents# mv spark-1.6.2-bin-hadoop2.6 /usr/local/spark/&#10;root@master:~/Documents# cd /usr/local/spark/&#10;root@master:/usr/local/spark# ls&#10;spark-1.6.2-bin-hadoop2.6&#10;root@master:/usr/local/spark# 2、 配置环境变量 修改~/.bashrc文件，添加SPARK_HOME，将spark的bin目录添加至PATH： 1root@master:~# cat .bashrc&#10;...&#10;# . /etc/bash_completion&#10;#fi&#10;export JAVA_HOME=/usr/lib/java/jdk1.8.0_91&#10;export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$SCALA_HOME/bin:$SPARK_HOME/bin:$PATH&#10;export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar&#10;export JRE_HOME=$&#123;JAVA_HOME&#125;/jre&#10;&#10;export HADOOP_HOME=/usr/local/hadoop/hadoop-2.4.6&#10;export SCALA_HOME=/usr/lib/scala/scala-2.10.6&#10;export SPARK_HOME=/usr/local/spark/spark-1.6.2-bin-hadoop2.6&#10;root@master:~# 保存并退出。使用source命令使配置文件生效。 3、 配置Spark 需要配置spark-env.sh文件和slaves文件，没有的话可以根据该目录下的template文件创建。 进入Spark安装目录下的conf目录，根据模板创建新文件。 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6# cd conf&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# ls&#10;docker.properties.template metrics.properties.template spark-defaults.conf.template&#10;fairscheduler.xml.template slaves spark-env.sh&#10;log4j.properties.template slaves.template spark-env.sh.template&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# cp spark-env.sh.template spark-env.sh&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# cp slaves.template slaves 修改spark-env.sh，在最后添加下面的信息。 保存退出。 最终效果如下： 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# cat spark-env.sh&#10;#!/usr/bin/env bash&#10;...&#10;export JAVA_HOME=/usr/lib/java/jdk1.8.0_91&#10;export HADOOP_HOME=/usr/local/hadoop/hadoop-2.4.6&#10;export SCALA_HOME=/usr/lib/scala/scala-2.10.6&#10;export HADOOP_CONF_DIR=/usr/local/hadoop/hadoop-2.7.2/etc/hadoop&#10;export SPARK_MASTER_IP=master&#10;export SPARK_WORKER_MEMORY=1g&#10;export SPARK_WORKER_CORES=1&#10;export SPARK_WORKER_INSTANCES=1&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# 上面各个参数的作用在文档中已经提及。可以自行查阅进行个性化配置。 修改slaves文件。 最终效果如下： 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# vim slaves&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# cat slaves&#10;#&#10;...&#10;# A Spark Worker will be started on each of the machines listed below.&#10;worker1&#10;worker2&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6/conf# 这里只设置两台worker主机为工作节点。 4、 配置其余主机 worker1和worker2的配置与master主机完全相同。可以通过SSH的scp命令将Spark的安装目录和~/.bashrc文件复制到两台worker主机。 这里不再赘述。 5、 启动并测试集群的状况 1） 当前我们只使用Hadoop的HDF文件系统，所以可以只启动master主机Hadoop的HDFS文件系统。进入Hadoop的sbin目录下，然后在shell命令终端输入./start-dfs.sh命令，可以看到master启动了namenodes，worker1和worker2都启动了datanode，表示HDFS文件系统已经启动。 1root@master:/usr/local/hadoop/hadoop-2.4.6# ./sbin/start-dfs.sh 这时候会启动上一篇文章中的部分服务。具体可以通过jsp查看。 2） 用Spark的sbin目录下的./start-all.sh命令启动Spark集群。 如果上面配置没有问题的话，就可以很顺利的开启服务了。 提示信息也很简洁。仅供对照。 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6# ./sbin/start-all.sh &#10;starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/spark-1.6.2-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.master.Master-1-master.out&#10;worker2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/spark-1.6.2-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-worker2.out&#10;worker1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/spark-1.6.2-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-worker1.out&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6# 6、 简单验证 1）使用JPS在master结点、worker1、2结点分别可以查看到新开启的Master和Worker进程。 master： 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6# jps&#10;3600 SecondaryNameNode&#10;3289 NameNode&#10;3914 Master&#10;3983 Jps&#10;root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6# worker1： 1root@worker1:~# jps&#10;12245 Worker&#10;11592 DataNode&#10;12299 Jps&#10;root@worker1:~# worker2： 1root@worker2:~# jps&#10;12019 Jps&#10;11325 DataNode&#10;11966 Worker&#10;root@worker2:~# 2）进入Spark的WebUI页面，访问master:8080，可以看到如下图所示的界面。 3）进入Spark的bin目录，使用spark-shell命令可以进入spark-shell控制台。 1root@master:/usr/local/spark/spark-1.6.2-bin-hadoop2.6# ./bin/spark-shell &#10;16/07/25 20:10:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&#10;16/07/25 20:10:30 INFO spark.SecurityManager: Changing view acls to: root&#10;16/07/25 20:10:30 INFO spark.SecurityManager: Changing modify acls to: root&#10;16/07/25 20:10:30 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)&#10;16/07/25 20:10:31 INFO spark.HttpServer: Starting HTTP Server&#10;16/07/25 20:10:31 INFO server.Server: jetty-8.y.z-SNAPSHOT&#10;16/07/25 20:10:31 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:46693&#10;16/07/25 20:10:31 INFO util.Utils: Successfully started service &#39;HTTP class server&#39; on port 46693.&#10;Welcome to&#10; ____ __&#10; / __/__ ___ _____/ /__&#10; _\ \/ _ \/ _ `/ __/ &#39;_/&#10; /___/ .__/\_,_/_/ /_/\_\ version 1.6.2&#10; /_/&#10;&#10;Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91)&#10;Type in expressions to have them evaluated.&#10;Type :help for more information.&#10;16/07/25 20:10:52 INFO spark.SparkContext: Running Spark version 1.6.2&#10;16/07/25 20:10:53 WARN spark.SparkConf: &#10;SPARK_WORKER_INSTANCES was detected (set to &#39;1&#39;).&#10;This is deprecated in Spark 1.0+.&#10;&#10;Please instead use:&#10; - ./spark-submit with --num-executors to specify the number of executors&#10; - Or set SPARK_EXECUTOR_INSTANCES&#10; - spark.executor.instances to configure the number of instances in the spark config.&#10; &#10;16/07/25 20:10:53 INFO spark.SecurityManager: Changing view acls to: root&#10;16/07/25 20:10:53 INFO spark.SecurityManager: Changing modify acls to: root&#10;...... 然后可以在WebUI页面输入master:4040从Web的角度了解spark-shell，也可以查看environment（注意此时不要退出或者关闭Spark-Shell，否则4040无法访问）。如下图所示： 到这里，Spark集群部署成功。下面将介绍集群的测试。 小结回顾一下上面的整体步骤：配置好Hadoop、Spark、Scala之后，首先通过start-dfs.sh开启Hadoop，然后通过start-all.sh开启Spark，之后运行spark-shell，即可查看4040。 关闭的顺序和上面相反。 错误提示如果执行完./sbin/start-dfs.sh后，再执行./sbin/start-all.sh的时候出错，如下： 1root@master:/usr/local/spark/spark-1.6.2# ./sbin/start-all.sh&#10;starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/spark-1.6.2/logs/spark-root-org.apache.spark.deploy.master.Master-1-master.out&#10;failed to launch org.apache.spark.deploy.master.Master:&#10; Failed to find Spark assembly in /usr/local/spark/spark-1.6.2/assembly/target/scala-2.10.&#10; You need to build Spark before running this program.&#10;full log in /usr/local/spark/spark-1.6.2/logs/spark-root-org.apache.spark.deploy.master.Master-1-master.out&#10;worker1: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/spark-1.6.2/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-worker1.out&#10;worker2: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/spark-1.6.2/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-worker2.out&#10;worker1: failed to launch org.apache.spark.deploy.worker.Worker:&#10;worker1: Failed to find Spark assembly in /usr/local/spark/spark-1.6.2/assembly/target/scala-2.10.&#10;worker1: You need to build Spark before running this program.&#10;worker1: full log in /usr/local/spark/spark-1.6.2/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-worker1.out&#10;worker2: failed to launch org.apache.spark.deploy.worker.Worker:&#10;worker2: Failed to find Spark assembly in /usr/local/spark/spark-1.6.2/assembly/target/scala-2.10.&#10;worker2: You need to build Spark before running this program.&#10;worker2: full log in /usr/local/spark/spark-1.6.2/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-worker2.out&#10;root@master:/usr/local/spark/spark-1.6.2# 问题的原因在于文章开始的时候提到的Spark版本和Hadoop版本不兼容。资料介绍说Hadoop版本迭代的时候某些组件变化较大，因此Spark版本最好选择spark-1.6.2-bin-hadoop2.6这种和Hadoop绑定的版本。 参考文章： Spark（一）：Spark的安装部署。 http://my.oschina.net/gently/blog/686192]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spark入门 - 1 搭建Hadoop分布式集群]]></title>
      <url>%2F2016%2F07%2F19%2FSpark%E5%85%A5%E9%97%A8%20-%201%20%E6%90%AD%E5%BB%BAHadoop%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%2F</url>
      <content type="text"><![CDATA[2016-07-25 更新： 进行到Spark安装的时候，spark-1.6.2没有对应的hadoop-2.6.4的with bin版本，因此推翻重来。 目前为hadoop-2.6.4+scala-2.10.6+spark-1.6.2-bin-hadoop2.6。 因此，如果近期搭建环境，注意三个包的版本选择搭配。官网有具体说明，请具体查阅。如果有精力，也可以使用源码等方式自行操作。 安装Ubuntu系统不论是通过虚拟机方式还是直接在物理机上安装Ubuntu系统，网上都有丰富的教程，此处不再赘述。 为了方便起见，此处设置的机器名最好与书本的设置一致：Master、Slave1和Slave2。 配置root用户登录这里有一步与普通教程不同。在安装好系统，重启之后，完成了相关配置。可以进行这一步，设置使用root用户登录，方便以后多个服务器相互操作。如下所示。 1&#20026;&#20102;&#31616;&#21270;&#26435;&#38480;&#38382;&#39064;&#65292;&#38656;&#35201;&#20197;root&#29992;&#25143;&#30340;&#36523;&#20221;&#30331;&#24405;&#20351;&#29992;Ubuntu&#31995;&#32479;&#12290;&#32780;&#22312;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;Ubuntu&#27809;&#26377;&#24320;&#21551;root&#29992;&#25143;&#65292;&#36825;&#37324;&#20570;&#30456;&#20851;&#37197;&#32622;&#12290;&#22312;&#21629;&#20196;&#32456;&#31471;&#36755;&#20837;sudo -s&#21629;&#20196;&#65292;&#28982;&#21518;&#26681;&#25454;&#25552;&#31034;&#36755;&#20837;&#23494;&#30721;&#65292;&#36827;&#20837;root&#29992;&#25143;&#26435;&#38480;&#27169;&#24335;&#12290; 如果是Ubuntu 12.04（本人机器）和书本对应的Ubuntu 12.10，或者能够找到/etc/lightdm/目录下的lightdm.conf文件，则按照下面的方法1操作；如果该文件不存在，则按照方法2操作。 方法1：在命令终端输入vim /etc/lightdm/lightdm.conf命令（或者在root权限下直接gedit/etc/lightdm/lightdm.conf），修改lightdm.conf的内容为： 1[SeatDefaults]&#10;greeter-session=unity-greeter&#10;user-session=ubuntu&#10;greeter-show-manual-login=true 即添加了一行greeter-show-manual-login=true。 此处可能需要安装vim，根据提示执行sudo apt-get install vim即可。 保存文件，然后退出。在命令终端输入命令passwd root，设置管理员用户密码，最好与普通用户一致。 1root@box:~# passwd root&#10;Enter new UNIX password: &#10;Retype new UNIX password: &#10;passwd: password updated successfully&#10;root@box:~# 然后重启，之后登录界面会多一个Login选项，输入账号名root和密码，即可使用root账户登录。 到这里，这部分的配置结束。 方法2：测试了一下最新的Ubuntu 16.04，发现上面的lightdm.conf文件不存在，然后按照网上的资料进行配置。 基本步骤完全一样：打开终端，输入命令sudo -s。 到这里可以先通过passwd root配置管理员账号密码。 然后编辑配置文件： 1gedit /usr/share/lightdm/lightdm.conf.d/50-ubuntu.conf 之后操作与上面一样，新增一行greeter-show-manual-login=true。 之后重启即可。 Error found when loading /root/.profile如果登录过程中提示： 1Error found when loading /root/.profile&#10;mesg: ttyname failed: Inappropriate ioctl for device&#10;As result the session will not be configured correctly.&#10;You should fix the problem as soon as feasible. 则启动系统后，提取权限到root用户，然后通过#vim /root/.profile修改/root/目录下的.profile文件，将其中的mesg n替换成tty -s &amp;&amp; mesg n。重启系统，问题解决。 重启ubuntu，问题解决 安装JDK参照之前的博客，按步骤来即可。 搭建另外两台Ubuntu系统并配置SSH免密码登录之前的系统安装和JDK安装都大同小异，不在赘述。VMware下可以直接复制虚拟机，会简单很多，不过稍后需要自行修改某些配置。 简单起见，系统和JDK的配置最好保持一致，如均采用root用户登录、JDK安装目录。 配置SSH免密码登录SSH的基本操作参照另一篇博客。配置成功即可。 Hadoop的Master和Slave结点之间的通信，以及Spark的Master和Worker结点之间的通信，都是通过SSH来完成的。为了避免以后每次通信都要输入密码，此处配置几台服务器相互SSH免密码登录。 注意下面均为在sudo -s之后的操作。 后续的3步主要目的在于将各个主机的公钥互存。每个主机都在authorized_keys文件中保存自己和其他所有主机的公钥，也就是每个主机的authorized_keys文件都是一样的。 生成公钥和私钥在终端中输入 1root@master:~# ssh-keygen -t rsa -P &#39;&#39; 命令，然后回车一次（如果不带-P参数，则连续回车三次），然后在/root/.ssh目录下已经生成公钥文件id_rsa.pub和私钥文件id_rsa。同样的在其他所有主机上进行相同的操作，生成各自的公钥、私钥文件。 追加公钥追加本机公钥将公钥id_rsa.pub追加到authorized_keys文件内（如果文件不存在则新建：touch authorized_keys）： 1root@master:~/.ssh# cat id_rsa.pub &#62;&#62; authorized_keys 此时authorized_keys文件内已经包含id_rsa.pub文件的内容了。 追加其他主机的公钥首先将其他主机的公钥通过scp命令复制到本机。以slave1主机为例，主机IP为192.168.247.131： 1root@slave1:~/.ssh# scp id_rsa.pub root@192.168.247.131:/root/.ssh/id_rsa.pub.slave1 可以通过ifconfig命令查看本机IP。 然后追加公钥： 1root@master:~/.ssh# cat id_rsa.pub.slave1 &#62;&#62; authorized_keys 其余主机操作类似。 共享authorized_keys文件此时主机master的authorized_keys文件已经包含了所有主机的公钥信息，也包括自己的。然后我们将master主机的authorized_keys文件复制到其余所有主机，相当于其他主机也都自己保存了所有主机的公钥。 1root@master:~/.ssh# scp authorized_keys root@:192.168.247.131:/root/.ssh/authorized_keys 更换IP即可复制到其他主机。 测试SSH登录1root@master:~# ssh 192.168.247.131 安装Hadoop和搭建Hadoop分布式集群前面的算是准备工作，这里才真正涉及到Hadoop相关配置及操作。 下面的主要任务是配置Hadoop的文件和系统的相关文件两个方面。可以单独对各台主机进行配置，内容都一样；也可以直接将master主机的配置文件通过scp命令复制到其他主机；另外，书本提到，配置服务器集群的时候，可以使用PSSH简化配置。 安装Hadoop1、下载Hadoop 在官网下载Hadoop，选择bin版本的压缩包，如hadoop-2.6.4.tar.gz，解压后将文件夹放在/usr/local/hadoop/目录下，最终效果为：/usr/local/hadoop/hadoop-2.6.4。 2、 配置hadoop-env.sh 在目录/usr/local/hadoop/hadoop-2.6.4/etc/hadoop下的文件hadoop-env.sh内配置JDK的安装信息。 显式的配置成下面这样： 1# The java implementation to use.&#10;export JAVA_HOME=/usr/lib/java/jdk1.8.0_91 3、 配置.bashrc 为了方便之后开机启动系统后可以立即使用Hadoop的bin目录下的命令，可以把Hadoop的bin目录配置到~/.bashrc文件中，修改后的配置如下： 1# . /etc/bash_completion&#10;#fi&#10;export JAVA_HOME=/usr/lib/java/jdk1.8.0_91&#10;export PATH=$JAVA_HOME/bin:/usr/local/hadoop/hadoop-2.6.4/bin:$PATH&#10;export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar&#10;export JRE_HOME=$&#123;JAVA_HOME&#125;/jre&#10;&#10;export HADOOP_HOME=/usr/local/hadoop/hadoop-2.6.4 主要为HADOOP_HOME和PATH的配置。 保存并退出。 使用source ~/.bashrc命令使配置信息生效。 使用hadoop version验证是否配置成功。 配置Hadoop分布式集群配置主机名及IP地址以master主机的操作为例。 1、 修改主机名。 在/etc/hostname文件内只有一行，修改为想要的主机名即可。此处是为了配置Spark集群，因此命名风格规范化。此处修改成master。 1root@master:~# cat /etc/hostname &#10;master&#10;root@master:~# 2、 主机名和IP的对应关系 修改/etc/hosts文件，在文件内添加三台主机的名字和IP地址。最终效果如下： 1root@master:~# cat /etc/hosts&#10;127.0.0.1&#9;localhost&#10;127.0.1.1&#9;Tbox&#10;192.168.247.130&#9;master&#10;192.168.247.131&#9;worker1&#9;&#10;192.168.247.132&#9;worker2&#10;&#10;# The following lines are desirable for IPv6 capable hosts&#10;::1 ip6-localhost ip6-loopback&#10;fe00::0 ip6-localnet&#10;ff00::0 ip6-mcastprefix&#10;ff02::1 ip6-allnodes&#10;ff02::2 ip6-allrouters&#10;root@master:~# 关键在于4~6行的配置。其中IP地址可以通过在终端中输入ifconfig命令查看。 如果修改后，主机名没有生效的话，重启系统再开机的时候会发现主机名已经改变。 3、 其余主机 其余两台主机的配置类似。可以手动配置，可也以复制过去。PSSH用法不熟悉，暂不介绍。 hostname文件内容不同，单独配置；hosts文件内容相同，可以直接复制。（由于上面的hosts文件内已经配置了主机名和IP的对应关系，此时可以直接以root@worker1的方式复制） 1root@master:/etc# scp hosts root@worker1:/etc/ Hadoop配置1、创建namenode和datanode目录 在/usr/local/hadoop/hadoop-2.6.4目录下新建四个目录：tmp、hdfs、hdfs/data、hdfs/name。 1root@master:/usr/local/hadoop/hadoop-2.7.2# mkdir tmp&#10;//&#26368;&#32456;&#25928;&#26524;&#22914;&#19979;&#10;.&#10;&#9500;&#9472;&#9472; bin&#10;&#9474; &#9500;&#9472;&#9472; ...&#10;&#9500;&#9472;&#9472; etc&#10;&#9474; &#9492;&#9472;&#9472; hadoop&#10;&#9500;&#9472;&#9472; hdfs&#10;&#9474; &#9500;&#9472;&#9472; data&#10;&#9474; &#9492;&#9472;&#9472; name&#10;&#9500;&#9472;&#9472; ...&#10;&#9492;&#9472;&#9472; tmp 2、 修改master的配置文件 下面主要修改/etc/hadoop下的core-site.xml、mapred-site.xml、hdfs-site.xml。如果没有该文件则新建，其中mapred-site.xml可以根据mapred-site.xml.template创建，即将mapred-site.xml.template复制为mapred-site.xml。 需要配置的内容为&lt;configuration&gt;标签内的属性。 core-site.xml： 1root@master:/usr/local/hadoop/hadoop-2.6.4# cat etc/hadoop/core-site.xml &#10;&#60;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&#62;&#10;&#60;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&#62;&#10;&#60;!--xxx--&#62;&#10;&#10;&#60;!-- Put site-specific property overrides in this file. --&#62;&#10;&#10;&#60;configuration&#62;&#10;&#9;&#60;property&#62;&#10;&#9;&#9;&#60;name&#62;fs.default.name&#60;/name&#62;&#10;&#9;&#9;&#60;value&#62;hdfs://master:9000&#60;/value&#62;&#10;&#9;&#60;/property&#62;&#10;&#9;&#60;property&#62;&#10;&#9;&#9;&#60;name&#62;hadoop.tmp.dir&#60;/name&#62;&#10;&#9;&#9;&#60;value&#62;/usr/local/hadoop/hadoop-2.6.4/tmp&#60;/value&#62;&#10;&#9;&#60;/property&#62;&#10;&#60;/configuration&#62;&#10;root@master:/usr/local/hadoop/hadoop-2.6.4# mapred-site.xml： 1root@master:/usr/local/hadoop/hadoop-2.7.2# cat etc/hadoop/mapred-site.xml&#10;&#60;?xml version=&#34;1.0&#34;?&#62;&#10;&#60;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&#62;&#10;&#10;&#60;!-- Put site-specific property overrides in this file. --&#62;&#10;&#10;&#60;configuration&#62;&#10;&#9;&#60;property&#62;&#10;&#9;&#9;&#60;name&#62;mapred.job.tracker&#60;/name&#62;&#10;&#9;&#9;&#60;value&#62;master:9001&#60;/value&#62;&#10;&#9;&#60;/property&#62;&#10;&#60;/configuration&#62;&#10;root@master:/usr/local/hadoop/hadoop-2.7.2# hdfs-site.xml： 1root@master:/usr/local/hadoop/hadoop-2.6.4# cat etc/hadoop/hdfs-site.xml &#10;&#60;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&#62;&#10;&#60;?xml-stylesheet type=&#34;text/xsl&#34; href=&#34;configuration.xsl&#34;?&#62;&#10;&#10;&#60;!-- Put site-specific property overrides in this file. --&#62;&#10;&#10;&#60;configuration&#62;&#10;&#9;&#60;property&#62;&#10;&#9;&#9;&#60;name&#62;dfs.replication&#60;/name&#62;&#10;&#9;&#9;&#60;value&#62;2&#60;/value&#62;&#10;&#9;&#60;/property&#62;&#10;&#9;&#60;property&#62;&#10;&#9;&#9;&#60;name&#62;dfs.name.dir&#60;/name&#62;&#10;&#9;&#9;&#60;value&#62;/usr/local/hadoop/hadoop-2.6.4/hdfs/name&#60;/value&#62;&#10;&#9;&#60;/property&#62;&#10;&#9;&#60;property&#62;&#10;&#9;&#9;&#60;name&#62;dfs.data.dir&#60;/name&#62;&#10;&#9;&#9;&#60;value&#62;/usr/local/hadoop/hadoop-2.6.4/hdfs/data&#60;/value&#62;&#10;&#9;&#60;/property&#62;&#10;&#60;/configuration&#62;&#10;root@master:/usr/local/hadoop/hadoop-2.6.4# 3、 masters和slaves文件 masters： 1root@master:/usr/local/hadoop/hadoop-2.7.2# cat etc/hadoop/masters &#10;master&#10;root@master:/usr/local/hadoop/hadoop-2.7.2# slaves： 1root@master:/usr/local/hadoop/hadoop-2.7.2# cat etc/hadoop/slaves &#10;worker1&#10;worker2&#10;root@master:/usr/local/hadoop/hadoop-2.7.2# 配置其他主机这部分的主要内容是重复上面的两步。 第一步的主机名和IP已经配置过了。可以按照之前的介绍独立配置或者复制过去。 第二步的Hadoop配置都是在hadoop-2.6.4目录下，因此可以考虑通过scp命令直接将整个文件夹复制到worker1和worker2。 1root@master:/usr/local/hadoop# scp -r hadoop-2.7.2/ root@worker1:/usr/local/hadoop/&#10;root@master:/usr/local/hadoop# scp -r hadoop-2.7.2/ root@worker2:/usr/local/hadoop/ 最后，配置其余主机的.bashrc文件，将Hadoop目录写进.bashrc文件。也可以将master文件直接复制过去。然后source .bashrc使配置文件生效。 测试Hadoop分布式集群1、 在master结点格式化集群的文件系统，在命令终端输入： 1root@master:/usr/local/hadoop/hadoop-2.7.2# hadoop namenode -format 2、 启动Hadoop集群 进入Hadoop的sbin目录，在终端输入命令（保证其他的主机已经开启）： 1root@master:/usr/local/hadoop/hadoop-2.7.2/sbin# ./start-all.sh &#10;This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh&#10;Starting namenodes on [master]&#10;master: starting namenode, logging to /usr/local/hadoop/hadoop-2.7.2/logs/hadoop-root-namenode-master.out&#10;worker2: starting datanode, logging to /usr/local/hadoop/hadoop-2.7.2/logs/hadoop-root-datanode-worker2.out&#10;worker1: starting datanode, logging to /usr/local/hadoop/hadoop-2.7.2/logs/hadoop-root-datanode-worker1.out&#10;Starting secondary namenodes [0.0.0.0]&#10;0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/hadoop-2.7.2/logs/hadoop-root-secondarynamenode-master.out&#10;starting yarn daemons&#10;starting resourcemanager, logging to /usr/local/hadoop/hadoop-2.7.2/logs/yarn-root-resourcemanager-master.out&#10;worker1: starting nodemanager, logging to /usr/local/hadoop/hadoop-2.7.2/logs/yarn-root-nodemanager-worker1.out&#10;worker2: starting nodemanager, logging to /usr/local/hadoop/hadoop-2.7.2/logs/yarn-root-nodemanager-worker2.out&#10;root@master:/usr/local/hadoop/hadoop-2.7.2# 3、 通过JPS命令分别查看一下三台主机的进程信息： master： 1root@master:/usr/local/hadoop/hadoop-2.7.2/sbin# jps&#10;4705 NameNode&#10;5049 SecondaryNameNode&#10;5274 Jps&#10;5198 ResourceManager&#10;root@master:/usr/local/hadoop/hadoop-2.7.2/sbin# worker1： 1root@worker1:~# jps&#10;3268 DataNode&#10;3485 NodeManager&#10;3631 Jps&#10;root@worker1:~# woker2： 1root@worker2:~# jps&#10;3600 Jps&#10;3425 NodeManager&#10;3189 DataNode&#10;root@worker2:~# 至此Hadoop句群构建完毕。 4、 查看master的Web控制台： 在master主机打开浏览器，查看Web控制台。网址为：http://localhost:50070或者http://localhost:50070/dfshealth.html#tab-overview。 5、 停止Hadoop集群 1root@master:/usr/local/hadoop/hadoop-2.7.2/sbin# ./stop-all.sh &#10;This script is Deprecated. Instead use stop-dfs.sh and stop-yarn.sh&#10;Stopping namenodes on [master]&#10;master: stopping namenode&#10;worker1: stopping datanode&#10;worker2: stopping datanode&#10;Stopping secondary namenodes [0.0.0.0]&#10;0.0.0.0: stopping secondarynamenode&#10;stopping yarn daemons&#10;stopping resourcemanager&#10;worker2: stopping nodemanager&#10;worker1: stopping nodemanager&#10;worker2: nodemanager did not stop gracefully after 5 seconds: killing with kill -9&#10;worker1: nodemanager did not stop gracefully after 5 seconds: killing with kill -9&#10;no proxyserver to stop&#10;root@master:/usr/local/hadoop/hadoop-2.7.2# 目前不清楚为什么出现最后的两句提示：nodemanager did not stop gracefully after 5 seconds: killing with kill -9，希望后面的学习过程中可以解决这个疑惑。 错误提示1、no datanode to stop 如果在执行stop-all.sh文件的时候提示“no datanode to stop”，可以根据下面的方法排除错误。 出现该错误的原因是因为每次使用hadoop namenode -format命令格式化文件系统的时候会出现一个新的namenodeId，而在之前的格式化过程中，在我们手动创建的tmp目录下已经有了数据，只需要把每台机器上tmp文件夹、hdfs文件夹下的data和name文件夹中的内容清空即可，同时清空系统/tmp文件夹下与Hadoop相关的内容清空。 master主机和多个worker主机的上述文件夹内容都要清空。 重新格式化并重新启动集群，即可解决问题。 2、运行start-all.sh错误 在开启Hadoop集群的时候，如果提示error等问题，需要停下来，根据错误提示排除问题。比如可能要手动修改上面的hadoop-env.sh文件中的export HADOOP_CONF_DIR=/usr/local/hadoop/hadoop-2.6.4/etc/hadoop来排除 1...&#10;Error: Cannot find configuration directory: /etc/hadoop&#10;Error: Cannot find configuration directory: /etc/hadoop&#10;... 错误。 其余错误问题根据具体提示查找资料排除即可。 以上，Spark搭建的前期部分已经完成。 参考文章： hadoop运维时遇到的一个简单问题 关闭进程时报错no 进程 to stop http://www.hello-code.com/group/Hadoop/201407/1283.html Ubuntu 15.04开机root登录 http://www.linuxdiyf.com/linux/13626.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[django后台入门--引言]]></title>
      <url>%2F2016%2F07%2F10%2Fdjango%E5%90%8E%E5%8F%B0%E5%85%A5%E9%97%A8--%E5%BC%95%E8%A8%80%2F</url>
      <content type="text"><![CDATA[写在前面最近面试Shanbay的暑期实习生，在正式开始面试之前要有一个小作业，根据布置的任务做出一个背单词网站Demo，供后面面试官筛选和筛选后的面试提问。然而作为一个刚入门的纯Java后台，Python很久之前学过一点，但很久不用了，Django不会，Bootstrap不会，Web不会（不过应该会写一个helloworld页面吧）。一脸懵逼啊。。。 经过十天左右的“奋战”，好歹做出了一个能跑起来的Demo：功能基本完成，逻辑还算完善，不过界面确实丑了一点。压力之下快速学习过这么一套知识，收获颇丰。特此记录，一方面帮助自己巩固知识点，记录一些踩过的坑；另一方面，希望有相似需求的人能看到这篇文章，少走弯路。 2016-07-11最新消息，挂掉。细节部分处理太过粗糙，很多部分我是当做作业来做，实现功能即可。放弃了一些编码规范和必要的函数解耦。总的来说，还是太嫩。 题目介绍题目具体要求：XXX。要求用Python完成，使用Django框架和bootstrap来辅助做页面。 分析按照以往经验，把整个业务逻辑先划分模块，理清脉络。针对小模块做开发，最终连贯成完整的业务逻辑。这里就不再根据题目进行具体的分析，通性基本为用户模块和针对具体应用的应用模块。 具体实现根据需求进行增加或者简化即可。 一些感悟DjangoDjango很强大，可以通过简单配置，实现很多强大的功能。比如后台管理系统，之前使用Spring一个人开发两周、还要有个Web端的小伙伴配合，呈现的效果还不如Django几行配置。当然，开发友好也是一方面，代码简单强大，谁用谁知道。不过相应的，高度封装带来的好处必然也会有对应的缺点。封装程度过高，需要微调的时候，会有束手束脚的感觉；迁移数据库的时候，万一操作不当，应该是再难以修复（血泪史）。客观问题确实存在一些，不过发挥主观能动性还是可以解决的。 Django的坑Django版本之间变化大，操作命令可能会迥异。按照某个教程入门，可能会出现各种因为Django版本不同而命令不同的问题，对新手的积极性挫伤很大。如之前版本中的syncdb命令，在本次的Django1.9.7版本中，已经不适用。现在只提供python managy.py migrate和python managy.py makemigrations命令。 因此本文的后续部分，可能也会在某天之后失效。 不过教程的作用不仅限于照本宣科，机械的学习别人给出的每一步。其实某一步出错的时候，可以重试或查阅资料后再解决之，举一反三，触类旁通更重要。必要时推翻重来熟悉一下整体步骤也会收获很大。 参考资料官方文档自然是最好的选择，不过英文阅读可能会是一些人的障碍。目前中文版的Django book中对应的Django版本距离1.9.7版本差距也较大，不过参照上一小节，很多操作稍作变通即可。从这里学习到很多，活学活用。 另外的其他一些参考资料在本文的最后附录给出，帮助很大，从中学到很多知识。 Bootstrap至今本人对bootstrap的掌握仅限于很粗糙的能使用，所以这里也没什么好介绍的。只是简单地使用栅格来做布局。 参考文章： 两篇文章帮你入门Django(上)——https://segmentfault.com/a/1190000002447511 两篇文章帮你入门Django(下)——https://segmentfault.com/a/1190000002479013 虫师：django 快速实现完整登录系统（cookie）——http://www.cnblogs.com/fnng/p/3750596.html PYTHON开发入门与实战5-DJANGO模型——http://www.cnblogs.com/haozi0804/p/4489063.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux环境下安装ssh]]></title>
      <url>%2F2016%2F07%2F01%2FLinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85ssh%2F</url>
      <content type="text"><![CDATA[前言在配置 Hadoop 集群分布时，要使用SSH免密码登录，spark也是。此处只简单介绍ssh的安装，后续的免密码登录在Spark配置文章中详细介绍、记录。 简单介绍维基百科定义： Secure Shell（缩写为 SSH），由 IETF 的网络工作小组（Network Working Group）所制定；SSH 为一项创建在应用层和传输层基础上的安全协议，为计算机上的 Shell（壳层）提供安全的传输和使用环境。 SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。SSH 有很多功能，它既可以代替 Telnet，又可以为 FTP、POP、甚至为 PPP 提供一个安全的 “通道”。 SSH 分为客户端和服务端。服务端是一个守护进程，一般是 sshd 进程，在后台运行并响应来自客户端的请求。提供了对远程请求的处理，一般包括公共密钥认证、密钥交换、对称密钥加密和非安全连接。客户端一般是 ssh 进程，另外还包含 scp、slogin、sftp 等其他进程。 SSH安装及配置SSH 分客户端 openssh-client 和 openssh-server好像是Ubuntu等Linux系统已经有了openssh-server服务，因此只需要安装ssh服务即可。 安装ssh按照王家林spark书本介绍，使用下面的命令：1# apt-get install ssh 这里是管理员账号，因此不需要使用sudo提升权限。普通用户注意在前面加上sodo。 或者使用下面的命令：1# apt-get install openssh-client 启动服务管理员权限下：1# /etc/init.d/ssh stop //&#20572;&#27490;&#10;# /etc/init.d/ssh start //&#21551;&#21160;&#10;# /etc/init.d/ssh restart //&#37325;&#21551; OR1# service ssh start&#10;# service ssh stop&#10;# service ssh restart 如果有警告提示的话，可以尝试第二种service方式。警告提示：1root@Tbox:~/.ssh# /etc/init.d/ssh start&#10;Rather than invoking init scripts through /etc/init.d, use the service(8)&#10;utility, e.g. service ssh start&#10;&#10;Since the script you are attempting to invoke has been converted to an&#10;Upstart job, you may also use the start(8) utility, e.g. start ssh&#10;ssh start/running, process 3717 即：1root@Tbox:~# service ssh start 此时则不会出现刚才的提示。 验证是否安装成功成功的提示如下：1root@Tbox:~/.ssh# ssh localhost&#10;root@localhost&#39;s password: &#10;Welcome to Ubuntu 12.04.5 LTS (GNU/Linux 3.13.0-32-generic x86_64)&#10;&#10; * Documentation: https://help.ubuntu.com/&#10;&#10;298 packages can be updated.&#10;251 updates are security updates.&#10;&#10;New release &#39;14.04.4 LTS&#39; available.&#10;Run &#39;do-release-upgrade&#39; to upgrade to it.&#10;&#10;&#10;Your Hardware Enablement Stack (HWE) is supported until April 2017.&#10;&#10;Last login: Wed Jun 29 13:55:07 2016 from localhost&#10;root@Tbox:~# 之后继续进行的所有的操作都是通过ssh登录本机进行的。 查看服务可以通过命令：1# ps -e|grep ssh 查看ssh相关服务有没有开启，或者针对性的关闭多余服务（#kill -9 3717） 其实上面第一种方式操作开启ssh服务的时候，服务已经启动起来了：1root@Tbox:~/.ssh# ps -e|grep ssh&#10;&#10; 2660 ? 00:00:00 ssh-agent&#10; 3717 ? 00:00:00 sshd 退出ssh直接在终端中输入exit即可 演示如下：1root@Tbox:~# ps -e|grep ssh&#10; 2660 ? 00:00:00 ssh-agent&#10; 3745 ? 00:00:00 sshd&#10; 3748 pts/0 00:00:00 ssh&#10; 3749 ? 00:00:00 sshd&#10;root@Tbox:~# exit&#10;logout&#10;Connection to localhost closed.&#10;root@Tbox:~/.ssh# ps -e|grep ssh&#10; 2660 ? 00:00:00 ssh-agent&#10; 3745 ? 00:00:00 sshd&#10;root@Tbox:~/.ssh# 配置修改/etc/ssh/sshd_config文件进行参数配置，具体配置根据需要查询资料即可。 之后可以配置公钥私钥，使服务器相互访问。 更多地资料可以查看官方文档，或者参考相关博客。 本文参考文章：http://www.cnblogs.com/rond/p/3688529.htmlhttp://www.cnblogs.com/xiazh/archive/2010/08/13/1798844.htmlhttp://jingyan.baidu.com/article/9c69d48fb9fd7b13c8024e6b.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux环境下安装Java]]></title>
      <url>%2F2016%2F06%2F30%2FLinux%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85Java%2F</url>
      <content type="text"><![CDATA[前言Linux系统中Java环境的配置真的是非常基础但又非常重要，而且因为非常基础，所以在配置新的Linux工作环境的时候，不可避免的就要把相关基础配置重复一遍，有点像是建房子的地基建设，不可避免要重复很多遍。之前一直是需要的时候临时查找资料，后面发展到本地做个文档记录下大致步骤，最近学习Spark，又一轮的环境配置及Linux一些基础工具的安装，之前散乱的知识点串联了一遍。刚好趁此机会，把以前想要整理、但一直拖拉没下手的零散知识点统一整理一遍，po出来做个记录。 这应该是本次记录的第一篇，多说几句废话。本文的一些前序工作，如Linux系统的安装，不论是虚拟机，还是双系统，都大同小异，而且参考资料已经很多了，因此不再记述。本文中的Linux系统是Ubuntu 12.04，64位，因此有些命令可能不适用于其他版本的系统，稍作变通即可。本文主要是做记录给自己看，因此其中的很多细节不会描述太过详细。 下面进入正文。 正文Java下载到oracle官网选择Java SE版本的JDK，然后根据系统及位数选择相应版本的安装包，如此处的jdk-8u91-linux-x64.tar.gz。这里我选择了最新版，也可以根据自己的需要下载特定版本的JDK。 解压缩及安装位置解压在Linux系统中，使用命令:1$ tar -zxvf jdk-8u91-linux-x64.tar.gz 解压即可。注意前面的$是系统提示符，具体使用中不要带入$符。 具体的解压缩命令又是可以重新开一篇文章讲解的事情。这里能用即可。 安装目录选择之前一直是将解压后的文件夹放置在系统/opt目录下，然后自己新建文件夹，如java，存放，因此最终目录如下：1/opt/java/jdk1.8.0_91 这次参照王家林的书中的方案，选择放置在/usr/lib/java目录下，因此最终效果如下：1/usr/lib/java/jdk1.8.0_91 在终端中进入刚刚解压时的目录，通过cp命令将解压后的文件夹复制过去或者使用mv命令也可以。不过mv操作失败的话就要重新解压，因此还是选择cp将文件夹复制过去。 命令如下：1$ cp -rf jdk1.8.0_91 /usr/lib/java/ 配置环境变量将目录信息以环境变量的形式写进配置文件，这里有两种方式，对应着两个不同的文件，一个是~/.bashrc，一个是/etc/profile。 写文件也有两种方式，一种是vim，一种是gedit。 gedit先说gedit。这种方式可以在终端Terminal中输入gedit ~/.bashrc或者gedit /etc/profile命令调用可视化的gedit工具写文件，之后直接复制粘贴即可，但很有可能出现权限不够的问题，不能保存文件。相较而言vim会好很多，一直在终端中操作，不过也会因为使用vim不熟练而引发很多问题。所以，去学学vim。 vim如果是新系统，需要安装vim，命令如下：1$ sudo apt-get install vim 如果需要卸载软件或者了解vim的基本编辑操作，查查其他博客资料，或者，去学学Linux和vim。 配置文件 写配置文件 在Linux终端中输入vim ~/.bashrc进入配置文件，把JDK的环境变量加入其中，保存并退出。文件内容如下：1#if [ -f /etc/bash_completion ] &#38;&#38; ! shopt -oq posix; then&#10;# . /etc/bash_completion&#10;#fi&#10;&#10;export JAVA_HOME=/usr/lib/java/jdk1.8.0_91&#10;export PATH=$JAVA_HOME/bin:$PATH&#10;export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar&#10;export JRE_HOME=$&#123;JAVA_HOME&#125;/jre 前面的#开头的语句是系统写入，后面的四句export是需要写入的内容。 使配置生效 然后在终端中输入source ~/.bashrc命令使配置文件生效。当然也可以直接重启系统使之生效，会很麻烦，不推荐。 查看配置是否有效 在终端中输入java -version查看刚刚配置的JDK版本。效果如下：1root@master:~# java -version&#10;java version &#34;1.8.0_91&#34;&#10;Java(TM) SE Runtime Environment (build 1.8.0_91-b14)&#10;Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode) 如果没有出现上述信息，则需要重新检查~/.bashrc文件配置。出现上述信息则表示JDK配置成功，本次工作完成。可以进行下一波任务了。 注 此处主要记录一些主要的步骤，而且主要功能是提醒自己，因此一些细节可能会显得模糊，见谅。另，关于使用普通用户还是root用户进行配置操作，以及采用不同配置方式后哪些用户可以使用此JDK的问题，此处不做记录。可以另行查阅资料解决。也许后面会做记录补充一下。 吐槽 强行开个5吐槽一下。王家林书本《Spark核心源码分析与开发实战》中介绍的Java配置语句只有下面两句：1export JAVA_HOME=/usr/lib/java/jdk1.8.0_91&#10;export JRE_HOME=$&#123;JAVA_HOME&#125;/jre 然后source ~/.bashrc，java -version，配置失败。还有一些小细节问题，这里就不吐槽了。 Over，到这里Linux下安装JDK完成，可以开始愉快的玩耍了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[记 2016 兴人类俱乐部中兴捧月-软件编程比赛]]></title>
      <url>%2F2016%2F06%2F26%2F%E8%AE%B0%202016%20%E5%85%B4%E4%BA%BA%E7%B1%BB%E4%BF%B1%E4%B9%90%E9%83%A8%E4%B8%AD%E5%85%B4%E6%8D%A7%E6%9C%88-%E8%BD%AF%E4%BB%B6%E7%BC%96%E7%A8%8B%E6%AF%94%E8%B5%9B%2F</url>
      <content type="text"><![CDATA[题目介绍这是在“2016 兴人类俱乐部中兴捧月”软件编程挑战赛中遇到的的题目之一，第二题是实现类似于“LRU”的算法，时间因素，没来的及做。 题目详情为了进行城市规划，需要计算一个居住区的住宅项目。该居住聚集区俯视图已经制作好，并划分成n x m个网格。如果某个网格单元具有屋顶的一部分，则向其赋值1，如果是空地，则赋值0。由值为1的相邻网格单元组成的簇认定为一个单独住宅。对角放置的值为1的网格则不被视为属于同一住宅屋顶。 类Homes的方法countHomes的输入应包括一个n x m阶的二维整数数组grid。其中，n和m分别表示输入数组grid的行数和列数。该方法应该返回一个表示住宅总数目的整数。grid只包含0和1。 有用的命令：使用length方法来计算二维数组的长度。语句：int row = arr.length;int col = arr[0].length;可分别将数组arr中的行数和列数保存在row和col中。 特别要求必须用Java语言实现此算法。需要提交完全有效的代码，而非部分正确且有效的代码。一旦提交，便无法再检查该题。所使用的JDK版本为1.7。 注：本题可以选择Java、C++等语言，此处选择Java之后才有的这个特别要求。 测试用例测试用例1：123456&#123; &#123;0,0,0,0&#125;, &#123;0,1,0,0&#125;, &#123;0,0,1,0&#125;, &#123;0,0,0,0&#125;&#125;; 结果：2。 测试用例2：1234567&#123; &#123;0,0,0,0,0&#125;, &#123;0,1,1,0,0&#125;, &#123;0,0,1,1,0&#125;, &#123;0,0,0,0,0&#125;, &#123;0,0,0,0,0&#125;&#125;; 结果：1。 题目分析思路整理分析题目可知，题目可以抽象为给定一个二维数组（数组元素只包含0或1），然后查找数组中连在一起的1的区域的个数，上下左右这类直线方式算有效连接，而斜线则不算。 测试用例1可以画图示意如下： 看图可知只有2行2列和3行3列的两个数组元素符合要求。测试用例2思路类似。 方法选择一种穷举才能解决的问题，要么嵌套循环来求解所有的可能情况，要么观察题目形式，找到适合的解题形式，比如此题的递归。 嵌套循环需要考虑的情况略微复杂，如某层循环每次循环时可能需要重置数组变量，以及两次循环之间的循环衔接、参数统计，最是问题的是时间复杂度会达到一个很高的量级，很繁琐。 因此考虑使用递归。 最近遇到的类似的测试题最终落脚点全都是递归，如咪咕的测试（往m个桶里倒n升的水），中兴本次测试之前的模拟测试题用的是百度实习生招聘的题目（单词首尾串联问题），本题也是，各大公司好像都很默契。 递归算法先贴代码：123456789101112131415void solve(int i,int j,int grid[][])&#123; if(i&gt;=0 &amp;&amp; i&lt;grid.length &amp;&amp; j&gt;=0 &amp;&amp; j&lt;grid[0].length) &#123; if(grid[i][j]==1) &#123; temp++; grid[i][j]=-1; solve(i,j+1,grid); solve(i+1,j,grid); solve(i,j-1,grid); solve(i-1,j,grid); &#125; &#125;&#125; 这里是采用了CSDN用户讨论中提出的一种方案（感谢），进行适当的变化。 代码算是比较简单，入参i和j标识此次递归的参数在数组的位置，grid数组是全局变量。每次遇到1的时候，temp参数则开始计数，同时被统计的1的位置设置为-1，便于下次递归。但是这里函数返回的是某次开始递归的连续的1的个数，而不是所有的连续1的区域数量；当继续递归找不到连续的1的时候，则退出，返回本次递归中统计的本区域内1的个数。 方法有些接近最终答案，不过需要进行稍微的改进，设置一些统计条件和改变一些递归条件：如统计区域总数而不是单个区域内连续的1的个数、重置为-1的数组元素不应继续递归，以及其他小问题。选择合适的截止条件很纠结，总是差强人意，之前实战的递归算法较少，缺乏经验，所以耽误了不少时间，还好最终拿出了一套方案。 最终版本的代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package simple;/** * @author Think * @since 2016-06-11 19:16:00 */public class Homes &#123; public static void main(String[] args) &#123; int a[][]= &#123; &#123;0,0,0,0&#125;, &#123;0,1,0,0&#125;, &#123;0,0,1,0&#125;, &#123;0,0,0,0&#125; &#125;;// int a[][]=// &#123;// &#123;0,0,0,0,0&#125;,// &#123;0,1,1,0,0&#125;,// &#123;0,0,1,1,0&#125;,// &#123;0,0,0,0,0&#125;,// &#123;0,0,0,0,0&#125;// &#125;; countHomes(a); &#125; public static int nCount=0; public static int temp=0; public static int countHomes(int grid[][])&#123; int row=grid.length; int col=grid[0].length; Homes h=new Homes(); for (int i=0;i&lt;row;i++)&#123; for (int j=0;j&lt;col;j++)&#123; if (grid[i][j]&lt;1)&#123; if (temp!=0)&#123; nCount++; temp=0; continue; &#125; &#125; h.solve(i,j,grid); &#125; &#125; System.out.println(nCount); return nCount; &#125; void solve(int i,int j,int grid[][]) &#123; if(i&gt;=0 &amp;&amp; i&lt;grid.length &amp;&amp; j&gt;=0 &amp;&amp; j&lt;grid[0].length &amp;&amp; grid[i][j]&gt;=0) &#123; if(grid[i][j]==1) &#123; temp++; grid[i][j]=-1; solve(i,j+1,grid); solve(i+1,j,grid); solve(i,j-1,grid); solve(i-1,j,grid); &#125; &#125; &#125;&#125; 代码不过多解释，怎么把递归函数和整体代码结合起来倒是耽误了很多的时间，其他没什么需要着重强调的。 简单注释 包simple，Homes函数，countHomes方法，需要本地实现的时候稍作修改即可。 其中静态方法以及递归不能设置成static的问题，通过new新建类的对象即可解决。 反思系统给定的两个测试用例本地测的时候全都通过了，但是提交上去的时候测试用例2输出结果居然不对，很费解。至今没发现问题所在。所以，欢迎发现问题的同学留言指正交流。谢谢~ 实践出真知，思路和方法在头脑中还算清晰，但是落地之后还是遇到不少或大或小的问题。多练手总是有好处的，mark。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[错误: 找不到或无法加载主类 simple.Test]]></title>
      <url>%2F2016%2F03%2F26%2F%E9%94%99%E8%AF%AF-%E6%89%BE%E4%B8%8D%E5%88%B0%E6%88%96%E6%97%A0%E6%B3%95%E5%8A%A0%E8%BD%BD%E4%B8%BB%E7%B1%BB-simple-Test%2F</url>
      <content type="text"><![CDATA[偶然的机会，碰到了一个小问题，虽然不难，却很折磨人；而且咨询度娘之后发现网友的解决方法千奇百怪，但是查看/尝试了大约5种不同的说法却没有能够解决问题。看来还是得去求助外网：bing OR google。后来谷歌之后解决了问题，但是却又碰到了新问题，此处做个记录：主要记录解决此问题的方法，顺便描述新问题的坑，留待后面填补。 问题描述环境描述脱离本机环境描述问题就是耍流氓。 OS：WIndows 10；Java Version：1.8.0_60，Java 8；Eclipse：Mars。 问题简介之前在eclipse里建立了一个名为simple_test项目，里面只有一个Test.java类，用来测试一些小的idea、验证有疑问的想法。正常编译运行都没问题，如果就此打住估计也没后面的故事了，也就不需要记录了。下午的时候手贱、同时也是为了给同学演示其他问题，手动删除了根目录下与src同级目录的bin目录：然后就悲剧了，回到eclipse下run as开始报错:错误: 找不到或无法加载主类 Test.java。不管是eclipse还是直接在该项目的根目录下运行java src\simple\Test.java都是报错。~~o(&gt;_&lt;)o ~~ 解决方案检查源码检查源码，查看是否有拼写错误、类名和文件名不同等诸如此类的人为错误。检查之后发现确实没有错误，毕竟是之前编译运行都能正常通过的。 检查JAR此方法还是蛮有道理的方案：项目右击 -build path -configure build path，检查Libraries标签下的jar包有没有报错，报错的根据需要定位到此错误、导入正确jar包，或直接删除。然而此方法并没有奏效，没有出现错误的jar包。 “解决”方法这里是真正有效解决问题的方法，但却没能完全打消心中的疑惑。在eclipse下的菜单栏：Project - Clean - Clean all projects -ok,bingo，问题解决。如果打开了很多项目的话，同时clean会浪费很多时间，可以选择Project - Clean - Clean projects selected below，然后单独选中项目，ok即可。 反思按理说这种问题直接在cmd中javac然后java也可以解决，但是在问题重现解决问题时发现win 10的系统环境配置和新版本的Java的配置不同于以往：win 10下直接根据提示安装jdk-8u60-windows-x64.exe到默认目录(一方面固态硬盘，另一方面每次折腾太心累，没重复以往的自定义安装软件目录)后，是可以直接打开eclipse编写代码并编译运行的。eclipse和IntelliJ IDEA在使用过程中未遇到过因Java和系统而产生的错误。查看系统变量-Path，发现win 10+jdk 8_60的组合中，自动生成的路径为：C:\ProgramData\Oracle\Java\javapath，进入目录发现有三个快捷方式，java.exe、javaw.exe、javaws.exe，定位发现均位于C:\Program Files\Java\jre1.8.0_60\bin下。一种不同于以往自定义方式的系统变量配置。新系统下之前从未配置过JAVA_HOME等信息，因而此处引出的javac命令调用不到、java环境配置缺失、win 10下系统环境变量等问题以及不知是win 10还是新版java的问题，新知识的涌现，暴露了自己的知识学习中的一个不大不小的坑，需要后续学习来填补。再次显示了自己蜡鸡的一面啊，==。欢迎能详细回答此问题的大神指点迷津。 参考文章感谢此文的原创博主，此文貌似已经看不出原作者是谁。不过非常认同作者静下心来思考问题的心态，自己动手排除问题，32个赞！参考文章：错误: 找不到或无法加载主类- 蓝讯]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring 源码深度解析-Spring源码导入]]></title>
      <url>%2F2016%2F03%2F01%2FSpring%20%E6%BA%90%E7%A0%81%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90-Spring%E6%BA%90%E7%A0%81%E5%AF%BC%E5%85%A5%2F</url>
      <content type="text"><![CDATA[本文主要介绍如何导入Spring源码及遇到的小问题。 絮叨最近准备开始逐步深入学习Spring源码，遂着手学习借阅已久的《Spring源码深度解析》这本书，但是到了第二章导入源码部分的时候碰到了小问题，本想借助度娘的力量轻松解决，但查阅资料都没能很好的解决我的问题（可能本人水平太低-_-|||，有些地方大神们以为很简单不值一提，一笔带过），然后结合大神博客里的方法，加上自己摸索算是解决了此问题吧。特在此处mark一下，记录此问题方便以后查阅，同时能对同样问题的小伙伴有所启发也是极好的。 导入项目按照书上的解释，同时编译整个Spring项目耗时耗资源，不建议采用，因此选择按照需求编译需要阅读源码的模块。 准备工作在安装JDK1.8之后，安装gradle（据说gradle比maven更有优势，心向往之）。Java安装配置不再赘述，gradle的配置也类似于maven，下载-解压-配置路径。具体此处不再描述。 下载Spring源码之后前往github下载Spring源码（或者通过git clone方式下载代码，当然前提是电脑里此时已经安装了github客户端或者类似的工具），可以选择下载mater分支，或者点击Tags选择release版本。 注：书上提供的git地址是springsource而不是此处的spring-project。应该是这里的为准。反正我是从这里下载的+_+。 编译及导入然后在本地解压项目压缩包，选择需要阅读的模块目录，编译即可。按照spring架构，spring-core、Beans等属于基础模块，书本推荐由浅入深了解，此处选择spring-beans，但beans模块依赖core，并且两者的问题均类似，缺少jar包，所以此处以beans举例说明问题。 在spring-core目录中点击上方的文件-打开命令提示符（或者win+R，然后输入cmd，回车，然后一步步cd切换至该目录）。ps：win10及之前的win8好像都支持这样操作。在命令窗口中输入gradle cleanidea eclipse即可编译该模块并导入到eclipse中（import-General-Existing project into Workspace）。 此时就出现本文的重点了：eclipse中项目上出现感叹号，然后项目右击-Build path-Config build path，在出现的第三个Libraries的tab中，发现提示错误，并且按照网上的很多方法没能很好的解决我的问题。 然后在Windows-show view-problems发现提示以下错误。 问题描述Project &#39;spring-core&#39; is missing required library: &#39;...spring-framework-master\spring-core\build\libs\spring-cglib-repack-3.2.0.jar&#39; Project &#39;spring-core&#39; is missing required library: &#39;...spring-framework-master\spring-core\build\libs\spring-objenesis-repack-2.2.jar&#39; The project cannot be built until build path errors are resolved 解决方法原因及下载jar包关于问题及出现的原因此文也有描述，请参考。通过阅读源码发现为了避免第三方class的冲突，spring把最新的cglib和objenesis给repack了参考文章：关于创建spring源码环境时遇到的缺少spring-cglib-repack-3.2.0.jar和spring-objenesis-repack-2.2.jar两个包的问题 及所需的jar包（非常感谢，不用非得消耗可恶的硬币）。spring-cglib-repack-3.2.0.jarspring-objenesis-repack-2.2.jar 按照错误提示，需要将jar包放入指定的文件夹。在spring-core目录下新建build文件夹，并在文件夹中新建lib文件夹，将下载的jar包放入文件夹内，最终如下： 下载及放置jar包都没问题，关键是后面怎么做，让我这菜鸟很是烦恼，百般尝试后终于解决（泪奔ing）。 配置回到刚才的项目右击-Build path-Config build path，然后在Libraries tab下选中出错的jar包，点击右侧的edit，切换目录至spring-core/build/lib/文件夹下，选中对应的刚刚放置好的jar包，两个jar包均这么做。然后apply，OK即可。 然而这样的话，后面的其他出现类似问题的项目每次都要这么解决，很是麻烦。希望有更好的解决方法的朋友分享你的方法，感谢。至此问题圆满解决。 后面是小插曲，觉得蛮有意思的，特此记录。 小插曲突然发现spring-framework-4.2.5.RELEASE版本居然没问题，之前在master分支直接下载的源码，进行完上述步骤后出现了下面的问题： 方法内部： 具体问题：错误1：错误2： Done2016年3月2号，下午两点，再去github查看master分支的源码，发现问题已经解决。 为大神们撒花~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F02%2F28%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. H1-testQuick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server123456$ hexo serveror$ hexo sor$ hexo s -p 4001# if port 4000 doesn't work More info: Server Generate static files1234$ hexo generateor$ hexo g# for short More info: Generating Deploy to remote sites1234$ hexo deployor$ hexo d# for short More info: Deployment H1-H2-testH1-test-another]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Notes]]></title>
      <url>%2F2016%2F02%2F25%2FNotes%2F</url>
      <content type="text"><![CDATA[更新-20161118前两天将最近学习Spark的心得体会记录成文的时候，突然发现github的博客一片空白。我勒个草，本地测试通过，编译部署都没问题，怎么会出现这种问题呢？费解。 查阅资料后发现很多人都有类似的问题，并且已经有人给出了很好的解决方案。就跟着别人的方案解决了问题，链接在这里。原因分析和解决思路都很清晰，赞。 看了下NexT主题更新之后确实很方便，添加了很多实用性新功能。然而本次更新仍然只是单纯地将功能迁移了一下，并未添加新功能。 P.S. 打赏功能鼠标悬浮的时候下面的字体晃来晃去，你们真的不觉得看着别扭吗？红红火火恍恍惚惚 不知不觉，NexT已经从0.5.0版本更新到了5.1.0，飞速发展，祝福越来越好。 TODO主题-Nexthttp://theme-next.iissnan.com/ 网页中有很多具体的参数设置，可以根据需要自行查阅更改配置。 TODOingFeed 不是很想做这部分多说的评论部分，不想做显示文章的阅读量主页和文章展开的时候在标题下方显示文章阅读人数。http://www.jeyzhang.com/hexo-next-add-post-views.html参照此处链接，可以实现。不过目前（2016-02-28）的hexo-next主题已经支持，只需要根据上述教程注册账号，填写好app-*信息即可。 如果刷新后，未达到效果，可等待几分钟再刷新即可。 测试代码显示效果123456789public class HelloWorld &#123; /** * 输出一行字符串“Hello World!” * @param args */ public static void main(String[] args) &#123; System.out.println("Hello World!"); &#125;&#125;]]></content>
    </entry>

    
  
  
</search>
